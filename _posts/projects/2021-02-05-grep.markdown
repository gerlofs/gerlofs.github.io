---
layout: front
title: "Greplike"
cover: "/assets/img/cowsay.png"
tags: unlisted

---

---

<h2 align="center">A pattern-matching file parser and regular expression engine</h2>
<h3>Weapon of choice:</h3>
<p>C</p>
<h3>Adapted from:</h3>
Greplike is an extension of <a href="https://www.oreilly.com/library/view/beautiful-code/9780596510046/ch01.html">Pike's regular expression matcher </a> and is, thus, also inspired by <a href="https://swtch.com/~rsc/regexp/">Russ Cox's series on regular expressions</a>.
<h3>Summary:</h3>

*Greplike* is a rudimentary, toy, pattern-matching tool that can be used, *like grep*, for parsing through files to find occurances of a string using literals or a regular expression. Unlike the *grep* tool, *greplike*
has some limitations including a lesser number of command-line and engine options available. For example: the user cannot invert the search using `-v` or ignore case using `-y` 
(greplike is **always** case-sensitive), and the user may not select a set of regular expressions to use (with *grep* this can be done with the `-p` and `-e` flags for PCRE and extended sets).
Greplike uses a sub-set of [POSIX Basic Regular Expressions](https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions) which does not (currently\*<sub>1</sub>) support the following
meta-characters:

- `{` and `}` (braces) used for matching a preceeding character/group/class *n* times.
- `[^` used for matching characters **not** within the suffix class, e.g. `[^abc]` matches everything besides *abc*.
- POSIX class categories (e.g. `[:upper:]` for all uppercase characters). 

Additionally greplike does not support metacharacters inside groups of classes (e.g. `(a+b+c)+d` is not a valid regular expression in greplike).

<h3>A quick, shallow dive into regular expression engines</h3>

If you're uncertain about what a regular expression is, I recommend reading Friedl's *Mastering Regular Expressions*. Essentially there are two viable ways of parsing and matching a text buffer
using regular expressions. Non-deterministic finite automata (NFA)-based matchers (regex enginers) rely heavily on *backtracking*\*<sub>2</sub> 

<h3>Appendix:</h3>
\*<sub>1</sub> | I, currently, have no plans to keep *greplike* up-to-date and implement upon the documented functionality. As stated, *greplike* was an exercise in learning about string parsing, regular 
expressions, and finite automata.

\*<sub>2</sub> | Backtracking is, essentially, a quality that arises in certain types of algorithmic problems such as constraint statisfaction problems (e.g. a sudoku solver) in which a successful
approach is based upon a condition that can sometimes be partially valid (e.g. Friedl presents the expression `to(nite|knight|night)` as an example, where testing and removing two of the three 
invalid options is required). A backtracked approach requires a variable to be stored (in this case the regular expression string) that can be reverted-to in a scenario where a partial match
eventually results in being invalid (e.g. `tonighz` will not match `to(nite|knight|night)` until the very last character check).


### What is the point in rewriting simplified versions of existing tools?

The goal of this rewrite project is to produce a tool that is robust and *at least* faster than matching using `strstr()`, with an end goal to produce results on par with regular expression pattern matching functionality available in interpreted languages such as Perl or Python.

### What is grep?

`Grep` is a pattern matching engine that routes matched lines of text to the standard output. Simply put:

> `grep` prints lines that contain a match for one or more _patterns_* [1]

*\* Emphasis is mine.*

The _grep_ command reads line from a file into a buffer until it reaches a new-line [2], searching each line for the input pattern and printing those lines back to the user. There is **a lot** more to grep than this due to the several quadrillion options available (see [1] and [3]), but this is the basics of Unix's fifteenth most important command [4].

### What are patterns and regular expressions?

_Patterns_ can be literal strings, e.g. `grep "article" article.md` will match the literal string `article` against each line and route that line to stdout if it contains an exact match [5], or regular expressions.
Regular expressions (*regex* for short) are a type of meta-programming-language consisting of literal strings and special characters that can be used to tell the parsing engine the byte content **and** conditions to match against.
For example, `grep "l" article.md` will match any line containing the character `l`, however, if we wanted to only match lines containing `l` at the beginning of the line, we could use a regular expression to fulfil this: `grep "^l" article.md`.

### Why grep?

Grep is described by Kernighan as "*the classic example of tool-based programming*" in *The Practice of Programming*\[X]. In the same book, an elegant solution of pattern-matching, with a more minimal subset of metacharacters than that used in grep, is presented (code by Rob Pike [X]). 
Kernighan later (in *Beautiful Code* [Y]) went on to applaud Pike's code and detailed why it was smaller and more pedagogical than the *grep* tool and its many variants. I think these points also echo why a recreation of *grep* serves as a valuable project in which to learn recursion, pointer arthimetic, and, arguably, the discipline of *clean code*:

> First, the features are well chosen to be the most useful and to give the most insight into implementation, without any frills. For example, the implementation of the anchored patterns ^ and $ requires only three or four lines, but it shows how to deal with special cases cleanly before handling the general cases uniformly. The closure operation * must be present because it is a fundamental notion in regular expressions and provides the only way to handle patterns of unspecified lengths. But it would add no insight to also provide + and ?, so those are left as exercises.

> Second, recursion is a win. This fundamental programming technique almost always leads to smaller, cleaner, and more elegant code than the equivalent written with explicit loops, and that is the case here. The idea of peeling off one matching character from the front of the regular expression and from the text, then recursing for the rest, echoes the recursive structure of the traditional factorial or string length examples, but in a much more interesting and useful setting.

> Third, this code really uses the underlying language to good effect. Pointers can be misused, of course, but here they are used to create compact expressions that naturally express the extracting of individual characters and advancing to the next character. Array indexing or substrings can achieve the same effect, but in this code, pointers do a better job, especially when coupled with C idioms for autoincrement and implicit conversion of truth values.

Additionally, a *grep*-like tool has the potential to teach the programmer a lot about testing. As Kernighan points out: 

> Regular expressions are rich enough that testing is far from trivial, but small enough that one can quickly write down a substantial collection of tests to be performed mechanically [Y]

That's a good few areas to dive in to despite the succinctness of Pike's original code - the _engine_ encompasses only 35 lines. The brevity of which allows for many improvements and additional 
features in the form of an expansion of supported metacharacters and command line options, many of which are even noted in the *Building on It* section of Kernighan's revist [Y].

### The uninteresting bit.

( Parse from standard input or a list of files )

### The regular expression engine.

### Testing

### Nondeterministic finite automata to solve pathological expressions

When I mentioned earlier in this article that regular expressions were originally designed as a notation for finite automata, you may have mistakenly thought that a restrictive, mathematical process was
originally developed and morphed into a more sophisticated and, more importantly, more efficient system. If you assumed this, sadly, you're incorrect. Pattern matching techniques in modern systems _can_ be 
considerably slower in edge cases than the original grep (and preceeding tools created by Thompson) designed and implemented more than 40 years ago [Z]. That's because the attention to the founding concept behind 
regular expressions was lost in translation somewhere along the line. The idea of finite automata (state machines) provides a way of thinking about regular expression pattern matching in both a deterministic
(DFA, deterministic finite automata) and non-deterministic (NFA, non-deterministic finitie automata) way. Deterministic state machines have a known outcome in each state, we know both where we currently are
and the possible states in which we can end up, inversely, non-deterministic state machines may have multiple edges that a traversable from a given state where a decision cannot be correct as we cannot *peek* ahead in the string.

NFAs and DFAs can be understood visually using a state graph, where circles represent a state, vertices are connected via edges which denote state transitions, and the final state is denoted by a double-walled circle.
The below example (A) shows a DFA for the regular expression `a` - basically, is there an a? If so, we go to the last state, otherwise there's no match and the check fails. Deterministic state machines are fundamentally easy
to understand once the diagramatic language is known, for a pattern of *n* length there may be *n*+1 states including the ultimate state. Simply, each item in the regular expression pattern forces through one state to the next 
in the case of a successful match, or produces a failure in the case where there can be no transition from one state to another or there are no more transitions (characters or metacharacters left in the pattern) and we 
are not at the final state. This is codified in Pike's regex engine, where each conditional check within the recursive functions denotes an edge between each state.

(A) <img src="/assets/img/project/grep/basic_machine">

In the case of regular exceptions displayed as deterministic state machines, each of the edges is a check against a character in the pattern. Metacharacters do not have their own edge in the same way, rather metacharacters alter the flow of the state
machine, as in the below case (B) where the `+` metacharacter performs an additional check for *b* after a sequence of two successive *b* characters, this route puts the state machine back to S<sub>1</sub> where 
a second *b* character is required for a successful check (eliminating strings with an odd number of *b* characters: e.g. *abbba* and *aba*). In a similar way, the `*` metacharacter provides an optional edge
which will force a move to a different state. In the example below (C) for the pattern `(a+b)*`, an edge allows a state change from the final state, where we have a match, to a state where we now need a *b* character
to reach the final state again.

(B) <img src="/assets/img/project/grep/cox_machine_steps">

(C) <img src="/assets/img/project/grep/asterisk_machine">

To make things ever-so-slightly more confusing, DFAs are NFAs and vice-versa. 

Converting regular expression patterns to NFAs...

Implementing NFA into pattern matching tools such as grep is one of the reasons *grep* and associated tools are much longer in code length than Pike's simplified tool - other reasons account for this too, such as 
a greater offering of metacharacters and functionality that does not conform to Thompson's construction (?). 




[Z Source](https://swtch.com/~rsc/regexp/regexp1.html)

*In progress...*

*Appendix:*

1. [Grep Manual](https://www.gnu.org/software/grep/manual/grep.html).

2. The grep command requires the end-of-file character be a new-line, it supplements if one is missing. The fact that `\n` is a required character for operation means that you *technically* cannot use it as an input pattern (see: 1, above). It [is possible](https://stackoverflow.com/questions/12652568/how-to-give-a-pattern-for-new-line-in-grep), however, to use `\n` in some scenarios thanks to the `-z` option.

3. [A decent summary of a few of the more useful options](https://www-users.york.ac.uk/~mijp1/teaching/2nd_year_Comp_Lab/guides/grep_awk_sed.pdf).

4. [Says this guy](https://www.oliverelliott.org/article/computing/ref_unix/).

5. This includes matches within words. Sticking with the example, a line containing the world `articles` will match as well as a line with `subarticles`.

3. [Mastering Regular Expressions](https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/) by Jeffrey Friedl.

3. Rob Pike's section on _Regex_ from [Beautiful Code](https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html). 
